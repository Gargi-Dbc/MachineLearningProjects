{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK_101.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEUaE54yGx7ZTcb5INc4bc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GargiChakraverty/MachineLearningProjects/blob/main/NLTK_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO3FZ_WDTnVu"
      },
      "source": [
        "Natural Language Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKp2aeGjNyim",
        "outputId": "ded519fd-a9e4-481d-a3a7-fa1450f30ffe"
      },
      "source": [
        "import nltk\n",
        "#importing a corpus(bag of words) from ntlk library\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKsYE0ZmUC4J",
        "outputId": "f1605ee5-5a54-45d0-b13c-86c372f8ce0e"
      },
      "source": [
        "brown.categories()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KG879UIUFQx",
        "outputId": "6d1b66e2-2ec5-4aa6-fe4c-5c2a0842d467"
      },
      "source": [
        "brown.words()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "VotA0adKcp4-",
        "outputId": "7a5f934e-eb9f-45cf-a16c-14e643475c96"
      },
      "source": [
        "data=brown.sents(categories=[\"adventure\"])\n",
        "print(len(data))\n",
        "print(data)\n",
        "\" \".join(data[205])"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4637\n",
            "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Gavin slipped his arms around his chest and hugged him fiercely .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCFUlwn3XQ0g"
      },
      "source": [
        "Tokensization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfgcFqDQXTjv",
        "outputId": "8f7d05ea-f247-4592-8804-5b298b72bc8c"
      },
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYCmCfgHXldG"
      },
      "source": [
        "document=\"\"\"It was a very good movie. The cast was amazing and I liked the story. I went to the movie hall to see it.\n",
        "\"\"\"\n",
        "sentence=\"NLP is awesome\""
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzZd9RDeXz4N",
        "outputId": "27ec1140-455c-46d1-82bd-aa4f14211242"
      },
      "source": [
        "#breaking down document into sentences \n",
        "sents=sent_tokenize(document)\n",
        "print(sents)\n",
        "#breaking down sentence into words\n",
        "words=word_tokenize(sentence)\n",
        "print(words)\n",
        "len(words)\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['It was a very good movie.', 'The cast was amazing and I liked the story.', 'I went to the movie hall to see it.']\n",
            "['NLP', 'is', 'awesome']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flMnhMW8Eka-"
      },
      "source": [
        "Stopword removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z-gQkJvEmRp",
        "outputId": "7d506f1b-6945-4ff8-ea7a-309faa7d9de1"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yM_3a5tEqk6"
      },
      "source": [
        "sw=set(stopwords.words('english'))"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T26X2YYcFAsm",
        "outputId": "351b66ee-0677-44e8-e296-75ff442b6483"
      },
      "source": [
        "print(sw)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'doing', 'she', 'how', 'in', 'further', 'haven', \"you'll\", 'y', \"couldn't\", 'didn', 'will', 'for', 'each', 'that', 'ain', 'yourself', 'we', 'are', 'on', 'again', 'very', 'i', 'it', 'same', 'then', 'too', 'these', 'd', \"you've\", 'here', 'between', 'until', 'an', 'before', 'some', \"it's\", 'only', 'when', 'don', 'o', 'where', \"should've\", 'be', 'weren', \"weren't\", 'wouldn', 'ours', 'up', 'itself', \"wouldn't\", 'needn', 'over', 'of', 'her', 'your', 'ma', 'once', 'themselves', 'does', \"shouldn't\", 'his', 'own', \"aren't\", 'nor', 've', 'couldn', 'had', 'both', 'than', 't', 'mightn', 'no', 'a', 'during', 'mustn', \"won't\", 'those', \"shan't\", 'above', 'was', 'there', 'should', 'as', 'now', \"hasn't\", 'yourselves', 'through', 'such', 'hers', 'my', 'm', 'off', 're', 'himself', \"haven't\", 'theirs', 'which', 'having', 'down', 'by', 'from', 'what', 'or', 'its', 'll', 'not', \"isn't\", 'most', 'after', 'did', 'against', 'been', 'to', 'hadn', \"mightn't\", 'who', 'why', 'herself', 'any', 'out', 'hasn', 'you', \"you'd\", 'whom', \"doesn't\", 'about', 'me', 'but', 'aren', \"mustn't\", 'ourselves', 'just', 'their', 'the', 'have', \"needn't\", 'our', 'they', 'them', 'this', 'being', 'because', 'shan', \"that'll\", 'wasn', 'isn', 'were', 'other', 'shouldn', 'into', 'has', 'can', 'so', \"she's\", 'below', 'am', 'he', 'while', 'few', 'and', 'do', 'at', \"you're\", \"hadn't\", 'all', 'him', 'under', 'doesn', \"didn't\", \"wasn't\", 'with', 'more', 'myself', 'won', 's', 'if', 'yours', 'is', \"don't\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aorZ1tDFaEj",
        "outputId": "6bfa89c7-55ce-44d5-8866-8cc25274a1f8"
      },
      "source": [
        "text=\"i am a not a very good guitar player\".split()\n",
        "print(text)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'am', 'a', 'not', 'a', 'very', 'good', 'guitar', 'player']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ul4-QWQjFhKL"
      },
      "source": [
        "#func to remove stopwords\n",
        "def remove_stopwords(text,stopwords):\n",
        "  useful_words=[w for w in text if w not in stopwords]\n",
        "  return useful_words"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H2NlgsUGbc1",
        "outputId": "02258a8d-72f0-4813-c1db-eadd6a926a5c"
      },
      "source": [
        "#calling the function on text and instance of stopwords\n",
        "useful_words=remove_stopwords(text,sw)\n",
        "print(useful_words)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['good', 'guitar', 'player']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-gahOo1QpYN"
      },
      "source": [
        "Tokenization using regex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEGeSsBVQ84D"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdpYm_12Qnpx",
        "outputId": "4ad238d4-1a2a-4df7-c91f-9b7537f0a983"
      },
      "source": [
        "sentence=\"my gmail is gargichakraverty@gmail.com\"\n",
        "tokenizer=RegexpTokenizer('[a-zA-Z@.]+')\n",
        "tokenised_text=tokenizer.tokenize(sentence)\n",
        "print(tokenised_text)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['my', 'gmail', 'is', 'gargichakraverty@gmail.com']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usjNF3bJRvHn"
      },
      "source": [
        "2.Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Qp4WnDRwv-"
      },
      "source": [
        "#nltk provides : Porter,Snowball and Lancaster stemmers\n",
        "from nltk.stem import SnowballStemmer ,PorterStemmer,LancasterStemmer"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Cwe-U_v1SzTu",
        "outputId": "3f097325-d142-46ea-841f-bbe864d61e14"
      },
      "source": [
        "ps=PorterStemmer()\n",
        "ps.stem('turning')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'turn'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTaE27MIUkEa"
      },
      "source": [
        "#SnowBallStemmer is multilingual(supports others languages)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2kKCMiVbx1"
      },
      "source": [
        "Making a vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRGhnIoRUu7-"
      },
      "source": [
        "corpus=['Dan Morgan told himself he would forget Ann Turner.',\n",
        "        'No girl would go this far to fool a man so she could kill him',\n",
        "        'Gavin slipped his arms around his chest and hugged him fiercely'\n",
        "]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Nx7eMOQ78y"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT2tViLiRLLC"
      },
      "source": [
        "cv=CountVectorizer()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9m0zRBXRNvJ",
        "outputId": "fa580d9f-89c2-4705-fe34-bbc03630b3b8"
      },
      "source": [
        "vc=cv.fit_transform(corpus)\n",
        "vc=vc.toarray()\n",
        "print(vc[1])\n",
        "print(vc)\n",
        "print(cv.vocabulary_)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1]\n",
            "[[0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1]\n",
            " [0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 1 0 0 1]\n",
            " [1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 2 1 0 0 0 0 0 1 0 0 0 0 0 0]]\n",
            "{'dan': 6, 'morgan': 21, 'told': 28, 'himself': 16, 'he': 14, 'would': 30, 'forget': 10, 'ann': 1, 'turner': 29, 'no': 22, 'girl': 12, 'go': 13, 'this': 26, 'far': 7, 'to': 27, 'fool': 9, 'man': 20, 'so': 25, 'she': 23, 'could': 5, 'kill': 19, 'him': 15, 'gavin': 11, 'slipped': 24, 'his': 17, 'arms': 2, 'around': 3, 'chest': 4, 'and': 0, 'hugged': 18, 'fiercely': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjKA0WQQRol6",
        "outputId": "9167155f-e505-4bbb-9a18-645386802d7b"
      },
      "source": [
        "numbers=vc[2]\n",
        "numbers"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXkGUBSDXXtq",
        "outputId": "e531acde-8d93-4802-9578-99c7a60784cd"
      },
      "source": [
        "#no of words in vocabulary\n",
        "len(numbers)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snnRd3TnXw57"
      },
      "source": [
        "Removing the stopwords while creating the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B8hgeZWX2v-"
      },
      "source": [
        "def myTokenizer(document):\n",
        "  words=tokenizer.tokenize(document.lower())\n",
        "  #removing stop words by calling above func\n",
        "  words=remove_stopwords(words,sw)\n",
        "  return words"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzfQEpyXZi-B",
        "outputId": "8a7408c2-78c0-4148-8fbd-e5df9d64392e"
      },
      "source": [
        "#applying above custom made tokenizer to corpus for creating new vocabulary with no stopwords\n",
        "cv=CountVectorizer(tokenizer=myTokenizer)\n",
        "vc=cv.fit_transform(corpus).toarray()\n",
        "print(cv.vocabulary_)\n",
        "#now length in decreased\n",
        "print(vc)\n",
        "len(vc[0])#earlier was 31"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dan': 5, 'morgan': 16, 'told': 18, 'would': 20, 'forget': 9, 'ann': 0, 'turner.': 19, 'girl': 11, 'go': 12, 'far': 6, 'fool': 8, 'man': 15, 'could': 4, 'kill': 14, 'gavin': 10, 'slipped': 17, 'arms': 1, 'around': 2, 'chest': 3, 'hugged': 13, 'fiercely': 7}\n",
            "[[1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1]\n",
            " [0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1]\n",
            " [0 1 1 1 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    }
  ]
}